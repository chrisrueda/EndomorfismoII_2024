{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir con una url\n",
    "url = \"https://raw.githubusercontent.com/chrisrueda/EndomorfismoII_2024/main/PROYECTOS/SEGUNDO%20PARCIAL/mora_cdto.csv\"\n",
    "# Cargar nuestros dataframe\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Contar las columnas numéricas y categóricas\n",
    "columnas_numericas = len(data.select_dtypes(include=['number']).columns)\n",
    "columnas_categoricas = len(data.select_dtypes(include=['object']).columns)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Número de columnas numéricas:\", columnas_numericas)\n",
    "print(\"Número de columnas categóricas:\", columnas_categoricas)\n",
    "\n",
    "#data.head(10)\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis estadistico simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todas las columnas que son de tipo float64\n",
    "float_columns = data.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Convertir las columnas de float64 a enteros\n",
    "data[float_columns] = data[float_columns].astype(int)\n",
    "print(float_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizar el balance de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso ideal ->50%(Positivo - 1) -50%(Negativo - 0)\n",
    "data[\"SP\"].unique() # Clase existente\n",
    "print(\"Balance: \")\n",
    "data[\"SP\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si las claves están presentes antes de acceder a ellas\n",
    "proporcion_si = data.get('Si', 1) / len(data)\n",
    "proporcion_no = data.get('No', 0) / len(data)\n",
    "\n",
    "# Mostrar la proporción de cada clase\n",
    "print(\"Proporción de 'Si':\", proporcion_si)\n",
    "print(\"Proporción de 'No':\", proporcion_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos nan o faltantes \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_faltantes = data.isnull()\n",
    "\n",
    "for i in datos_faltantes.columns.values.tolist():\n",
    "    print(datos_faltantes[i].value_counts())# Datos Anomalos o Cero\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos Anomalos o Cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular estadísticas resumidas para identificar valores anómalos\n",
    "summary_stats = data.describe()\n",
    "\n",
    "# Encontrar valores anómalos (por ejemplo, valores extremadamente altos o bajos)\n",
    "outliers = summary_stats.loc[['min', '25%', '75%', 'max']]\n",
    "\n",
    "# Encontrar valores que sean iguales a cero\n",
    "zeros = (data == 0).sum()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Valores anómalos:\")\n",
    "print(outliers)\n",
    "print(\"\\nValores iguales a cero:\")\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputacion con la media y mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mod = data.copy()\n",
    "data_mod.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir columnas a tipos numéricos\n",
    "#data_numerico = data_mod.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convertir datos categóricos en numéricos utilizando codificación one-hot\n",
    "data_numerico = pd.get_dummies(data_mod)\n",
    "\n",
    "# Convertir todos los datos negativos a positivos\n",
    "data_positivo = data_numerico.abs()\n",
    "print(data_positivo)\n",
    "\n",
    "# Mostrar los primeros registros del nuevo DataFrame con datos numéricos\n",
    "print(data_numerico.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorico\n",
    "mas_freq = data_mod[\"G\"].value_counts().idxmax()\n",
    "print(f\"La variable mas frecuente para G es {mas_freq}\")\n",
    "#imputar la variable\n",
    "data_mod[\"G\"].replace(np.nan,mas_freq,inplace=True)\n",
    "#Comprobar faltantes\n",
    "data_mod[\"G\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorico\n",
    "mas_freq = data_mod[\"EC\"].value_counts().idxmax()\n",
    "print(f\"La variable mas frecuente para EC es {mas_freq}\")\n",
    "#imputar la variable\n",
    "data_mod[\"EC\"].replace(np.nan,mas_freq,inplace=True)\n",
    "#Comprobar faltantes\n",
    "data_mod[\"EC\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorico\n",
    "mas_freq = data_mod[\"SP\"].value_counts().idxmax()\n",
    "print(f\"La variable mas frecuente para SP es {mas_freq}\")\n",
    "#imputar la variable\n",
    "data_mod[\"SP\"].replace(np.nan,mas_freq,inplace=True)\n",
    "#Comprobar faltantes\n",
    "data_mod[\"SP\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos Anomalos o Cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar qué datos son iguales a cero\n",
    "zeros_mask = data == 0\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Valores iguales a cero:\")\n",
    "print(zeros_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las categorías a valores numéricos (1 o 0)\n",
    "data_numerico = data_positivo.replace({'True': 0, 'False': 1})\n",
    "\n",
    "# Mostrar el nuevo DataFrame con valores numéricos\n",
    "print(data_numerico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generacion de modelos (Luego de la imputacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'CU', 'G', 'ED', 'EC', 'E', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6',\n",
      "       'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'P1', 'P2', 'P3', 'P4', 'P5', 'P6',\n",
      "       'SP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/chrisrueda/EndomorfismoII_2024/main/PROYECTOS/SEGUNDO%20PARCIAL/mora_cdto.csv')\n",
    "\n",
    "# Verificar las columnas presentes en el DataFrame\n",
    "print(data.columns)\n",
    "\n",
    "# Convertir las variables categóricas en variables dummy\n",
    "data = pd.get_dummies(data, columns=['G', 'EC'])\n",
    "\n",
    "# Separar las características (X) y la variable objetivo (y)\n",
    "X = data.drop(\"SP\", axis=1)\n",
    "y = data[\"SP\"]\n",
    "\n",
    "# Inicializar LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Convertir la variable objetivo a valores numéricos\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Inicializar el modelo de Bosque Aleatorio\n",
    "modelo_rf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Matriz de entrada\n",
    "X = data.drop(\"SP\", axis=1)\n",
    "# Target - Vector de salida\n",
    "y = data[\"SP\"]\n",
    "\n",
    "# Generar nuestro conjunto de entrenamiento y validacion \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "# Crear el modelo\n",
    "modelo_rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "# Entrenar el modelo\n",
    "modelo_rf.fit(X_train, y_train)\n",
    "# Validar el modelo (prediccion)\n",
    "y_pred = modelo_rf.predict(X_test)\n",
    "# Exactitud\n",
    "exactitud = accuracy_score(y_test, y_pred)\n",
    "print(f'Exactitud: {exactitud:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convertir las etiquetas de clase en valores numéricos\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Crear y entrenar el modelo XGBoost\n",
    "modelo_xgb = XGBClassifier()\n",
    "modelo_xgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Realizar predicciones sobre los datos de prueba\n",
    "y_pred_xgb = modelo_xgb.predict(X_test)\n",
    "\n",
    "# Calcular la exactitud\n",
    "exactitud_xgb = accuracy_score(y_test_encoded, y_pred_xgb)\n",
    "\n",
    "# Imprimir la exactitud\n",
    "print(f'Exactitud del modelo XGBoost: {exactitud_xgb:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Definir los parámetros a ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "# Inicializar el modelo XGBoost\n",
    "modelo_xgb = XGBClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar la búsqueda en cuadrícula para encontrar la mejor combinación de hiperparámetros\n",
    "grid_search = GridSearchCV(estimator=modelo_xgb, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones con el mejor modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calcular las métricas de evaluación\n",
    "con_mat = confusion_matrix(y_test_encoded, y_pred)\n",
    "exactitud = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred, pos_label=1)\n",
    "recall = recall_score(y_test_encoded, y_pred, pos_label=1)\n",
    "f1 = f1_score(y_test_encoded, y_pred, pos_label=1)\n",
    "\n",
    "# Calcular la especificidad\n",
    "vn, fp, fn, vp = con_mat.ravel()\n",
    "especificidad = vn / (vn + fp)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"Matriz de confusion:\\n{con_mat}\")\n",
    "print(f\"Exactitud: {exactitud:.2f}\")\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Sensibilidad: {recall:.2f}\")\n",
    "print(f\"Especificidad: {especificidad:.2f}\")\n",
    "print(f\"f1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no correr porque los datos dan cero \n",
    "#y_pred_str = ['Si' if pred == 1 else 'No' for pred in y_pred]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convertir las predicciones enteras en etiquetas de cadena\n",
    "y_pred_labels = np.where(y_pred == 0, 'No', 'Si')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print(f\"Matriz de confusión:\\n{conf_matrix}\")\n",
    "\n",
    "# Calcular las métricas\n",
    "exactitud = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels, pos_label='Si')\n",
    "sensibilidad = recall_score(y_test, y_pred_labels, pos_label='Si')\n",
    "especificidad = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "f1 = f1_score(y_test, y_pred_labels, pos_label='Si')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Exactitud: {exactitud:.2f}\")\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Sensibilidad: {sensibilidad:.2f}\")\n",
    "print(f\"Especificidad: {especificidad:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(con_mat)\n",
    "\n",
    "plt.xlabel(\"Variables predichas\")\n",
    "plt.ylabel(\"Verdad terreno\")\n",
    "sns.heatmap(con_mat, annot=True, fmt=\"d\", cmap=\"Blues\", square=True, cbar=False)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convertir las predicciones enteras en etiquetas de cadena\n",
    "y_pred_labels = np.where(y_pred == 0, 'No', 'Si')\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print(f\"Matriz de confusión:\\n{conf_matrix}\")\n",
    "\n",
    "# Calcular las métricas\n",
    "exactitud = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels, pos_label='No')\n",
    "sensibilidad = recall_score(y_test, y_pred_labels, pos_label='No')\n",
    "especificidad = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "f1 = f1_score(y_test, y_pred_labels, pos_label='No')\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(f\"Exactitud: {exactitud:.2f}\")\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Sensibilidad: {sensibilidad:.2f}\")\n",
    "print(f\"Especificidad: {especificidad:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(con_mat)\n",
    "\n",
    "plt.xlabel(\"Variables predichas\")\n",
    "plt.ylabel(\"Verdad terreno\")\n",
    "sns.heatmap(con_mat, annot=True, fmt=\"d\", cmap=\"Blues\", square=True, cbar=False)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluación de modelo\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# guardar modelo en el disco\n",
    "filename = 'P2_examen_MireyaT.sav'\n",
    "pickle.dump(modelo_rf, open(filename, 'wb'))\n",
    "# Cargar modelo del disco\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "#print(f\"Exactitud: {exactitud:.2f}\\nPrecisión: {precision:.2f}\\nSensibilidad: {recall:.2f}\\nEspecificidad: {especificidad:.2f}\\nf1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.read_csv('id.csv')\n",
    "vect = indices['ID'].to_list()\n",
    "\n",
    "data_mireya = data.loc[vect]\n",
    "data_m = data_mireya[['G_F','G_M','G_O','EC_0', 'EC_Casado','EC_Otros', 'EC_Soltero']]\n",
    "data_m.to_csv(\"datos_mireya.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              int64\n",
       "CU              int64\n",
       "ED              int64\n",
       "E               int64\n",
       "M1              int64\n",
       "M2              int64\n",
       "M3              int64\n",
       "M4              int64\n",
       "M5              int64\n",
       "M6              int64\n",
       "D1            float64\n",
       "D2            float64\n",
       "D3            float64\n",
       "D4            float64\n",
       "D5            float64\n",
       "D6            float64\n",
       "P1            float64\n",
       "P2            float64\n",
       "P3            float64\n",
       "P4            float64\n",
       "P5            float64\n",
       "P6            float64\n",
       "SP             object\n",
       "G_F              bool\n",
       "G_M              bool\n",
       "G_O              bool\n",
       "EC_0             bool\n",
       "EC_Casado        bool\n",
       "EC_Otros         bool\n",
       "EC_Soltero       bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
